---
authors: [ Valerio Galano ]
title: "Perché non possiamo fidarci dell'Intelligenza Artificiale: allucianzioni, sicofantia e menzogne"
layout: episode
episode_type: full
series: [ ]
categories: [ Podcast ]
tags: [ intelligenza artificiale ]
season: 2
number: 142
date: Sun, 17 Aug 2025 04:00:00 +0200
audio: "episodes/PIC142.mp3"
audio_duration: 2508
audio_size: 40127424
transcript: transcripts/PIC142.srt
url: /episodes/142-perche-non-possiamo-fidarci-dell-intelligenza-artificiale-allucianzioni-sicofantia-e-menzogne
aliases:
  - /142
  - /episodes/142
image: "covers/PIC142.png"
chapters: "chapters/PIC142.json"
explicit: "false"
draft: false
type: podcast
sources:
  - url: "https://www.ibm.com/think/topics/ai-hallucinations"
  - url: "https://openai.com/it-IT/index/introducing-gpt-4-5"
  - url: "https://www.technologyreview.com/2024/06/18/1093440/what-causes-ai-hallucinate-chatbots"
  - url: "https://arxiv.org/abs/2401.11817"
  - url: "https://www.theatlantic.com/technology/archive/2025/05/sycophantic-ai/682743"
  - url: "https://arxiv.org/abs/2310.13548"
  - url: "https://www.anthropic.com/research/reasoning-models-dont-say-think"
  - url: "https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf"
  - url: "https://arstechnica.com/science/2024/10/the-more-sophisticated-ai-models-get-the-more-likely-they-are-to-lie"
  - url: "https://www.nature.com/articles/s41586-024-07930-y"
  - url: "https://www.uniladtech.com/news/ai/sam-altman-trust-chatgpt-eerie-statement-hallucination-claims-782637-20250625"
supporters:
  - Edoardo Secco
  - Carlo Tomas
  - Michele S.
  - Paola Z.
soundbites:
  - start: "628.0"
    duration: "7"
    title: "Il compromesso tra creatività e accuratezza"
  - start: "1752.0"
    duration: "50.0"
    title: "Perché le IA imparano a mentire"
---

In questo episodio esploriamo i motivi per cui dobbiamo mantenere un atteggiamento critico verso l'Intelligenza Artificiale generativa. Analizziamo tre fenomeni fondamentali: le allucinazioni (quando i modelli producono informazioni false ma convincenti), la sicofantia (la tendenza ad assecondare sempre l'utente anche quando ha torto) e le menzogne (quando i modelli nascondono i loro veri processi di ragionamento). Scopriamo perché questi comportamenti sono intrinseci al funzionamento stesso dei Large Language Model e come il metodo di addestramento basato su feedback umano contribuisca a questi problemi. Un episodio essenziale per chiunque utilizzi strumenti di IA nella vita quotidiana o professionale.

[Pensieri in codice](https://pensieriincodice.it/142)

